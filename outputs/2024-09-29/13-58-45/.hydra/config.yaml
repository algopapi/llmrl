cache_dir: ~/.cache/huggingface/hub
huggingface_token: ''
wandb_key: ''
policy_lm: gpt2
critic_lm: roberta-base
agent: Archer
use_baseline: false
use_lora: false
max_new_tokens: 32
save_freq: 25
eval_freq: 25
capacity: 100000
rollout_size: 128
eval_size: 32
batch_size: 8
iterations: 2000
epochs: 50
actor_epochs: 3
warmup_iter: 20
grad_accum_steps: 32
do_sample: true
temperature: 1.0
critic_lr: 2.0e-05
lm_lr: 1.0e-06
env_idx: null
gamma: 0.95
tau: 0.1
max_grad_norm: 1.0
use_wandb: true
checkpoint_path: /global/scratch/users/yifeizhou/20q/twenty_questions_gpt2_model0_full.pt
save_path: /global/scratch/users/yifeizhou/20q/archer_20q_15
env_name: twenty_questions
env_load_path: /global/scratch/users/yifeizhou/20q/20q_t5_oracle.pt
project_name: llm_rl_20qsubset
run_name: archer-acc
